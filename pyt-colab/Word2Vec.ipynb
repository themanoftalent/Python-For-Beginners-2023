{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tokenlestirme.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWKxHt5beLKZO0iY65vd5z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/themanoftalent/pyt-colab/blob/master/Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaD6x24KJ8bP",
        "outputId": "de739658-6594-4359-8a77-5110869ff793"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "#project gutenberg\n",
        "#common crawl"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvf9K8MOVpkE"
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3qrlPxeVt2z"
      },
      "source": [
        "stop_wording=stopwords.words('english')"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAXwn8DmV5Hd"
      },
      "source": [
        "stop_wording"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwchQIfQV74F"
      },
      "source": [
        "# word2vec unsupervised \n",
        "# GloVe\n",
        "\n",
        "# skip-gram ortadaki kelimeden kenardaki kelimeyi tahmin eder. daha yavas\n",
        "#continuous bag of words (CBOW) KENARDAN ortayi tahmin et. daha buyuk corpus\n",
        "\n",
        "#skip-gram"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YojgL0poW9GL"
      },
      "source": [
        "wordsih=\"This will give you a list of rabbbit and lion and wolf tuple, name. If this list is not exactly what you want, it is certainly easier to parse the list you want out of this list then an nltk tree. Code and details from this link; check it out for more information You can also continue by only extracting the words, with the following function\""
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ0NzNSFXXwR"
      },
      "source": [
        "#Gensim ile word2vec"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGi9EHCBZsYs"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqidRKrtZ2kv"
      },
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieiWHA8oZ6gk"
      },
      "source": [
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNA0tsVcaB_v"
      },
      "source": [
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ9w5-DBaJIv"
      },
      "source": [
        "with open('Zafer Hoca oneriler.txt','r',encoding='utf-8') as f:\n",
        "  text=f.read()"
      ],
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "P5l2wcsTapuu",
        "outputId": "1027e0e5-e7cf-4a51-bff8-bf20083a1452"
      },
      "source": [
        "text"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Değerli Mehmet, \\n\\nTezinizle ilgili proje önerisini yeni inceleme olanağım oldu. Ellerinize sağlık. Dosyaları sağ olsun proje yürütücümüz ve  ABD Başkanımız, doktora tez izleme jürisi üyemiz Prof. Dr. Ali Hocamızla paylaşıyorum. Hocamızın da değerli öneri ve katkılarını bekliyoruz. Benim bazı önerilerim aşağıdaki gibi, tekrar okumak isterim. Ayrıca TTPYO birimimizden de öneri alarak, son haline ulaştırabiliriz kısa süre içinde.\\n\\nÖneriler:\\n\\n- İsmi Derin Öğrenme ve Ayrık Dalgacık Dönüşümü Yöntemleri ile Üst Solunum ve Akciğer Anomalilerinin Saptanması\\n\\n\\u200bgibi olabilir. Bu ara COVIT19 çok popüler. Belki böylelikle öncelikli ele alınabilir(?) diye düşündüm.\\n\\n- Tüm şekil ve çizelge noları kontrol edelim, (1. aşama var , 2. yok sayfa 7, 8, de... Sayfa 8 de şek. no kontrol edelim), \\n- Proje özetinde sayısal değerler belirttiğimizde kaynak no verelim, örneğin %80... gibi bilgiler kaynakla desteklenmeli.. \\nikinci paragrafta AlexNet yöntemi uygulacaktır... gibi gelecek zaman kullanalım, veya uygulanması planlanmaktadır  gibi...\\n\\n- Herhangi bir kısaltmayı mutlaka ilk geçtiği yerde parantez içinde açıklayalım sonra tekrar açıklamayalım. Rakamsal açıklama, formul, yorum vb. \\nönemli ifatelerden önce, sonra mutlaka kaynak verelim. \\n\\n- Kaynak taramamız az, biraz sayıyı artıralım, listeleyelim ekte, Türkiye'de yapılmış önemli yayınları, doktora /master tezi vb. ekleyelim. \\n- Derin öğrenme şeması bizim orjinal şemamız mı, yoksa kaynaktan mı aldık, kaynak verelim. mutlaka...\\n-şekiller mutlaka okunaklı olmalı, örneğin sayfa 8, Şekil 12 olacak sanrım, daha büyük olabilir, \\n-Şekil 13 ne gösteriyor, bizim ön bulgular mı? pilot çalışmamızdan bahsedebiliriz, bunun bir vaka analizi örneklem olduğunu iyice vurgulayalım... \\n-Şekil 14, 15 görselliği daha net olalım,\\n- Çizelge 4 de, aşama 1 aşama 2'den önce kolon olarak görünse... \\n- 11.sayfada ara rapor hazırlanması\\nyayın çalışmaları gibi konuları tabloya yerleştirelim. \\nAyrıca her bir proje üyesinin ne yapacağı ile ilgili görv ayfamız olmalı. 6.2'yi Değreli Ali Hocamız ve ben doldurup, yollayacağız.\\nBütçe oluşturalım, bir çalıştay düzenleyebiliriz, (1 sanal, 1 yüzyüze.. ):)\\n\\nTekrar hayırlı olsun, ellerinize sağlık, iyi dilekler, sağlıcakla kalınız. \\n\\nZ ASLAN\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeWNSkB0azrH"
      },
      "source": [
        "the_list =text.split('\\n')"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RrN0tmAa86F"
      },
      "source": [
        "corpus=[]\n",
        "for txt in the_list:\n",
        "  corpus.append(txt.split())"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bl4i8-ha9w5",
        "outputId": "bf7a90b5-7fb4-44cf-d043-a576eaad7454"
      },
      "source": [
        "print(corpus)"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Değerli', 'Mehmet,'], [], ['Tezinizle', 'ilgili', 'proje', 'önerisini', 'yeni', 'inceleme', 'olanağım', 'oldu.', 'Ellerinize', 'sağlık.', 'Dosyaları', 'sağ', 'olsun', 'proje', 'yürütücümüz', 've', 'ABD', 'Başkanımız,', 'doktora', 'tez', 'izleme', 'jürisi', 'üyemiz', 'Prof.', 'Dr.', 'Ali', 'Hocamızla', 'paylaşıyorum.', 'Hocamızın', 'da', 'değerli', 'öneri', 've', 'katkılarını', 'bekliyoruz.', 'Benim', 'bazı', 'önerilerim', 'aşağıdaki', 'gibi,', 'tekrar', 'okumak', 'isterim.', 'Ayrıca', 'TTPYO', 'birimimizden', 'de', 'öneri', 'alarak,', 'son', 'haline', 'ulaştırabiliriz', 'kısa', 'süre', 'içinde.'], [], ['Öneriler:'], [], ['-', 'İsmi', 'Derin', 'Öğrenme', 've', 'Ayrık', 'Dalgacık', 'Dönüşümü', 'Yöntemleri', 'ile', 'Üst', 'Solunum', 've', 'Akciğer', 'Anomalilerinin', 'Saptanması'], [], ['\\u200bgibi', 'olabilir.', 'Bu', 'ara', 'COVIT19', 'çok', 'popüler.', 'Belki', 'böylelikle', 'öncelikli', 'ele', 'alınabilir(?)', 'diye', 'düşündüm.'], [], ['-', 'Tüm', 'şekil', 've', 'çizelge', 'noları', 'kontrol', 'edelim,', '(1.', 'aşama', 'var', ',', '2.', 'yok', 'sayfa', '7,', '8,', 'de...', 'Sayfa', '8', 'de', 'şek.', 'no', 'kontrol', 'edelim),'], ['-', 'Proje', 'özetinde', 'sayısal', 'değerler', 'belirttiğimizde', 'kaynak', 'no', 'verelim,', 'örneğin', '%80...', 'gibi', 'bilgiler', 'kaynakla', 'desteklenmeli..'], ['ikinci', 'paragrafta', 'AlexNet', 'yöntemi', 'uygulacaktır...', 'gibi', 'gelecek', 'zaman', 'kullanalım,', 'veya', 'uygulanması', 'planlanmaktadır', 'gibi...'], [], ['-', 'Herhangi', 'bir', 'kısaltmayı', 'mutlaka', 'ilk', 'geçtiği', 'yerde', 'parantez', 'içinde', 'açıklayalım', 'sonra', 'tekrar', 'açıklamayalım.', 'Rakamsal', 'açıklama,', 'formul,', 'yorum', 'vb.'], ['önemli', 'ifatelerden', 'önce,', 'sonra', 'mutlaka', 'kaynak', 'verelim.'], [], ['-', 'Kaynak', 'taramamız', 'az,', 'biraz', 'sayıyı', 'artıralım,', 'listeleyelim', 'ekte,', \"Türkiye'de\", 'yapılmış', 'önemli', 'yayınları,', 'doktora', '/master', 'tezi', 'vb.', 'ekleyelim.'], ['-', 'Derin', 'öğrenme', 'şeması', 'bizim', 'orjinal', 'şemamız', 'mı,', 'yoksa', 'kaynaktan', 'mı', 'aldık,', 'kaynak', 'verelim.', 'mutlaka...'], ['-şekiller', 'mutlaka', 'okunaklı', 'olmalı,', 'örneğin', 'sayfa', '8,', 'Şekil', '12', 'olacak', 'sanrım,', 'daha', 'büyük', 'olabilir,'], ['-Şekil', '13', 'ne', 'gösteriyor,', 'bizim', 'ön', 'bulgular', 'mı?', 'pilot', 'çalışmamızdan', 'bahsedebiliriz,', 'bunun', 'bir', 'vaka', 'analizi', 'örneklem', 'olduğunu', 'iyice', 'vurgulayalım...'], ['-Şekil', '14,', '15', 'görselliği', 'daha', 'net', 'olalım,'], ['-', 'Çizelge', '4', 'de,', 'aşama', '1', 'aşama', \"2'den\", 'önce', 'kolon', 'olarak', 'görünse...'], ['-', '11.sayfada', 'ara', 'rapor', 'hazırlanması'], ['yayın', 'çalışmaları', 'gibi', 'konuları', 'tabloya', 'yerleştirelim.'], ['Ayrıca', 'her', 'bir', 'proje', 'üyesinin', 'ne', 'yapacağı', 'ile', 'ilgili', 'görv', 'ayfamız', 'olmalı.', \"6.2'yi\", 'Değreli', 'Ali', 'Hocamız', 've', 'ben', 'doldurup,', 'yollayacağız.'], ['Bütçe', 'oluşturalım,', 'bir', 'çalıştay', 'düzenleyebiliriz,', '(1', 'sanal,', '1', 'yüzyüze..', '):)'], [], ['Tekrar', 'hayırlı', 'olsun,', 'ellerinize', 'sağlık,', 'iyi', 'dilekler,', 'sağlıcakla', 'kalınız.'], [], ['Z', 'ASLAN']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYzTg1oCbP8t"
      },
      "source": [
        "model=Word2Vec(corpus,size=10,window=5,min_count=2,sg=1)"
      ],
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS4nKDqIbizk",
        "outputId": "20434984-6acb-48e1-f35f-71879ccc2529"
      },
      "source": [
        "model.vector_size"
      ],
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "VkiLXXeZbqUT",
        "outputId": "899a5eb6-d580-49e7-edcb-9b4d91f5d552"
      },
      "source": [
        "model.wv.most_similar('tez')"
      ],
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-269-360d0c25d108>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tez'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'tez' not in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adAO2-4Ccw2z"
      },
      "source": [
        "model.save('wordvec.model')"
      ],
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_RYbQN0dQIA"
      },
      "source": [
        "model=Word2Vec.load('wordvec.model')"
      ],
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIf36H0mdVr1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}